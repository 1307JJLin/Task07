# Task07
一 优化算法进阶
什么是ill-condition？如何判断出现了ill-condition?有什么危害？
学习率决定了优化的快慢

如何应对ill-condition？
什么是Momentum方法(SGD Momentum)，其数学原理是什么？(hint:指数加权移动平均)
有哪些自适应学习率方法？
AdaGrad:大的梯度处，小的学习率，即让最终的步长差不多大。此算法的缺点是，st随更新次数累加到一定程度会梯度为0，不再更新，小的学习率容易收敛到非极值点，大的学习率又是非常危险的，因为无法确定实际问题中的优化曲面长啥样。

RMSProp：对AdaGrad做了修改，并没有对历史梯度一直累加，而是用了指数加权移动平均，过早的梯度会被遗忘掉，解决了AdaGrad梯度消失的问题。

AdaDelta：与RMSProp不同的是，无需使用者指定学习率，而是算法计算出来的，此方法只有一个超参数

Adam：Momentum和RMSProp的结合体，并对EMA权重进行了无偏操作。良好性质有        ，但也带来一些可能不收敛的问题，有的论文专门剖析了

            Adam对大小相差很大数量级的梯度都可以rescale到相近的大小（Adam算法中的 mt​ 和 vt​ (原文符号)分别是梯度的一阶矩和二阶矩估计，二者相比，可以使更新量rescale到1的附近。）

             mt​ 和 vt​ 均使用了EMA（指数移动平均Exponential Moving Average），但是二者的衰减参数并不相同

 

二 word2vec
相比于使用 one-hot 向量表示词语，embedding的优点和缺点是什么？
优点：训练好的词向量中能够包含更多语义信息，词向量的维度是可以自由设定的（本质上说，词嵌入是一个降维操作）

缺点：词嵌入模型首先需要在大规模语料库上进行训练，才能得到更有意义的词向量，其次在后续模型的训练过程中，可能还需要进行进一步的模型参数优化，所以在实现和使用上，都是比 one-hot 向量更复杂的

 

词嵌入模型的训练本质上是在优化模型预测各词语同时出现的概率，其他word2vec的相关总结可移步Word2Vec原理

三 词嵌入进阶
一些对word2vec的改进算法

子词嵌入(fastText)
全局向量的词嵌入(GloVe)
GloVe 使用了非概率分布的变量，并添加了中心词和背景词的偏差项，这样做是在松弛概率的规范性，即各个概率事件的概率和加起来等于1

GloVe 使用了一个单调递增的权重函数来加权各个损失项

在有些情况下，交叉熵损失函数有劣势，GloVe 使用了平方损失函数

GloVe 的损失函数计算公式中用到了语料库上的全局统计信息
